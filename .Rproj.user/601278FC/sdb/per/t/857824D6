{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Capstone4.6\"\nauthor: \"emil_d\"\ndate: \"6 sierpnia 2017\"\noutput: html_document\n---\n\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n## Load libraries\nLoad libraries and set env.\n```{r echo = FALSE}\n#load libraries\nlibrary(tm)\nlibrary(ngramrr)\nlibrary(RWeka)\n\n\n#set homedir\nhomeDir <- \"d:/Kursy/Coursea/capstone_dinal/capstone_final\"\nsetwd(homeDir)\n\n#set random\nset.seed(7309)\n\n```\n\n## Load and clean data\n```{r}\n\n#attention - files en_US.blogs.txt, en_US.news.txt, en_US.twiter.txt must be putted\n#in homedir or directory path mus be adjusted in file section.\n\n#function for load files.\nloadFile <- function(fileName){\n  fileName <- file(fileName, open = \"rb\")\n  cData <- readLines(fileName, skipNul = TRUE)\n  close(fileName)\n  cData\n}\n\n\n#function for sampling. Variable sampleCount defines percentage of data (15% is the best value in accordance to speed and quality) \ngetSample <- function(cData){\n  sampleCount <- .15\n  cData <- sample(cData, length(cData)*sampleCount)\n  return(cData)\n}\n\n#function for cleaning data - gathered from the StackExchange :)\ncleanData <- function (cData) {\n  cData <- tm_map(cData, content_transformer(tolower))\n  cData <- tm_map(cData, stripWhitespace)\n  cData <- tm_map(cData, removePunctuation)\n  cData <- tm_map(cData, removeNumbers)\n  removeURLs <- function(tData) gsub(\"http[[:alnum:]]*\", \"\", tData) \n  cData <- tm_map(cData, content_transformer(removeURLs))\n  cData <- tm_map(cData, removeWords, stopwords(\"english\"))\n  return(cData)\n}\n\n\n#loadFile function use. Please adjust path to files if it is neccessary\nblogs <- loadFile(\"en_US.blogs.txt\")\nnews <- loadFile(\"en_US.news.txt\")\ntwitter <- loadFile(\"en_US.twitter.txt\")\n\n#merging data\nvData <- c(getSample(blogs), getSample(news), getSample(twitter))\n\n#writing into text file for cleaning\nwriteLines(vData, \"./data/data.txt\")\n\n#remove unneccessary variables\nrm(blogs, news, twitter, vData)\n\n#load text file\nvData <- VCorpus(DirSource(\"./data\"))\n\n#call to function cleanData\nvData <- cleanData(vData)\n\n#save vData as RDS file\nsaveRDS(vData, \"vData.rds\")\n\n```\n\n##Create n-Grams\n\n```{r}\n\n#tokenizer function\ncreateNgram <- function(tempData, n) {\n          NgramTokenizer <- function(x) {RWeka::NGramTokenizer(x, x = RWeka::Weka_control(min = n, max = n))}\n          tempData <- TermDocumentMatrix(tempData, control = list(tokenizer = NgramTokenizer))\n          return(tempData)\n}\n\n#sort function\nngramSort <- function (tempData) {\n  tempData <- as.matrix(tempData)\n  tempData <- as.data.frame(tempData)\n  colnames(tempData) <- \"Count\"\n  tempData <- tempData[order(-tempData$Count), , drop = FALSE]\n  return(tempData)\n}\n\n#change directory\nsetwd(\"./data\")\n\n#loop for n-grams creation\nfor (i in 1:4){\n  \n  #create filename with proper number\n  fileName <- paste(\"ngram\",i,\"sort.rds\", sep =\"\")\n  #call n-gram function\n  ngram <- createNgram(vData, i)\n  #call sort function\n  ngramsort <- ngramSort(ngram)\n  #save ngram file \n  saveRDS(ngramsort, fileName)\n  #clean unneccessary values\n  rm(ngram, ngramsort, fileName)\n}\n\n#create 4-gram    \nngram4sort <- readRDS(\"ngram4sort.rds\")\nfourGram <- data.frame(rows=rownames(ngram4sort), count=ngram4sort$Count)\nfourGram$rows <- as.character(fourGram$rows)\nfourGramSplit <- strsplit(as.character(fourGram$rows),split=\" \")\nfourGram <- transform(fourGram,first = sapply(fourGramSplit,\"[[\",1),second = sapply(fourGramSplit,\"[[\",2),third = sapply(fourGramSplit,\"[[\",3), fourth = sapply(fourGramSplit,\"[[\",4))\nfourGram <- data.frame(unigram = fourGram$first,bigram = fourGram$second, trigram = fourGram$third, quadgram = fourGram$fourth, freq = fourGram$count,stringsAsFactors=FALSE)\nwrite.csv(fourGram[fourGram$freq > 1,],\"./fourgram.csv\",row.names=F)\nfourGram <- read.csv(\"./fourgram.csv\",stringsAsFactors = F)\nsaveRDS(fourGram,\"./fourgram.rds\")\nrm(fourGram, fourGramSplit, ngram4sort)\n\n#create 3-gram\nngram3sort <- readRDS(\"ngram3sort.rds\")\nthreeGram <- data.frame(rows=rownames(ngram3sort),count=ngram3sort$Count)\nthreeGram$rows <- as.character(threeGram$rows)\nthreeGramSplit <- strsplit(as.character(threeGram$rows),split=\" \")\nthreeGram <- transform(threeGram,first = sapply(threeGramSplit,\"[[\",1),second = sapply(threeGramSplit,\"[[\",2),third = sapply(threeGramSplit,\"[[\",3))\nthreeGram <- data.frame(unigram = threeGram$first,bigram = threeGram$second, trigram = threeGram$third, freq = threeGram$count,stringsAsFactors=FALSE)\nwrite.csv(threeGram[threeGram$freq > 1,],\"./threegram.csv\",row.names=F)\nthreeGram <- read.csv(\"./threegram.csv\",stringsAsFactors = F)\nsaveRDS(threeGram,\"./threegram.rds\")\nrm(threeGram, threeGramSplit, ngram3sort)\n\n#create 2-gram\nngram2sort <- readRDS(\"ngram2sort.rds\")\ntwoGram <- data.frame(rows=rownames(ngram2sort),count=ngram2sort$Count)\ntwoGram$rows <- as.character(twoGram$rows)\ntwoGramSplit <- strsplit(as.character(twoGram$rows),split=\" \")\ntwoGram <- transform(twoGram,first = sapply(twoGramSplit,\"[[\",1),second = sapply(twoGramSplit,\"[[\",2))\ntwoGram <- data.frame(unigram = twoGram$first,bigram = twoGram$second,freq = twoGram$count,stringsAsFactors=FALSE)\nwrite.csv(twoGram[twoGram$freq > 1,],\"./twogram.csv\",row.names=F)\ntwoGram <- read.csv(\"./twogram.csv\",stringsAsFactors = F)\nsaveRDS(twoGram,\"./twogram.rds\")\nrm(twoGram, twoGramSplit, ngram2sort)\n\n```\n\n#Copy model into shiny\n\n```{r}\n#file copy\nfile.copy(\"./data/twogram.rds\", \"./word_predict\")\nfile.copy(\"./data/threegram.rds\", \"./word_predict\")\nfile.copy(\"./data/fourgram.rds\", \"./word_predict\")\n```\n\n\n",
    "created" : 1504275365655.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "72505475",
    "id" : "857824D6",
    "lastKnownWriteTime" : 1504276621,
    "last_content_update" : 1504276621074,
    "path" : "D:/Kursy/Coursea/capstone_final/capstone_final/capstone4.1.Rmd",
    "project_path" : "capstone4.1.Rmd",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}